###This is just an example of why it is imporatant to think about multiple comparisons.


###Simulate some observed data from a normal distribution that we want to model
y<-rnorm(1000,5,1)
hist(y)

###Now simulate an explanatory variable, completely unrelated to y
Var1<-rnorm(1000,4,1)
plot(Var1,y)

###You can see from the plot that variables y and Var1 are completely unrelated.
###If we were to do a linear model to test this relationship, we hopefully should
###find a p-value > 0.05 (this says there is no linear relationship between y and Var1)

summary(lm(y~Var1))

###If we repeat this code over and over again, sometimes we will get a p-value < 0.05!

y<-rnorm(1000,5,1)
Var1<-rnorm(1000,4,1)
summary(lm(y~Var1))

###Repeat the above code until you find a p-value <0.05

###In fact, we expect this to happen around 5% of the time.  So if we repeated the code 100 times, how many 
###times would we expect to see a p-value < 0.05?

p.values.too.small<-rep(0,100)
for (i in (1:100))
{
	y<-rnorm(1000,5,1)
	Var1<-rnorm(1000,4,1)
	mod<-lm(y~Var1)
	if (summary(mod)$coefficients[2,4] < 0.05) {p.values.too.small[i]<-1}
}
sum(p.values.too.small)/100

###hopefully this prints out a value near 0.05!

###What we want to individual compare y to around 15 variables instead of just 1?

y<-rnorm(1000,5,1)
Var1<-rnorm(1000,4,1)
Var2<-rnorm(1000,4,1)
Var3<-rnorm(1000,4,1)
Var4<-rnorm(1000,4,1)
Var5<-rnorm(1000,4,1)
Var6<-rnorm(1000,4,1)
Var7<-rnorm(1000,4,1)
Var8<-rnorm(1000,4,1)
Var9<-rnorm(1000,4,1)
Var10<-rnorm(1000,4,1)
Var11<-rnorm(1000,4,1)
Var12<-rnorm(1000,4,1)
Var13<-rnorm(1000,4,1)
Var14<-rnorm(1000,4,1)
Var15<-rnorm(1000,4,1)

###The above code simulates some reponse data y and 15 varaibles Var1,...,Var15.  We know for certain that 
###y isn't related to any of the 15 variables, for example

plot(y,Var9)

###Now lets carry out the 15 linear models.
###We know that we shouldn't find any significant relationships to concluded that y isn't realted to 
###any of the variables.

p.vals.too.small<-rep(0,15)

DATA<-data.frame(y,Var1,Var2,Var3,Var4,Var5,Var6,Var7,Var8,Var9,Var10,Var11,Var12,Var13,Var14,Var15)
for (j in (1:15))
{
	if (summary(lm(y~DATA[,j+1],data=DATA))$coefficients[2,4] < 0.05) {p.vals.too.small[j]<-1}
}

sum(p.vals.too.small)

###This value will tell you how many significant relationships you have found in this one study.  If it is more
###than zero then that is too many!

###Say 100 people repeat this same experiment.  Out of the 100 experiments, we would expect aaround 5 to give at least
###one significant result.  To find the proportion of people who do find a significant result, run the following code:

P.VALS<-rep(0,100)

for (i in (1:100))
{
	y<-rnorm(1000,5,1)
	Var1<-rnorm(1000,4,1)
	Var2<-rnorm(1000,4,1)
	Var3<-rnorm(1000,4,1)
	Var4<-rnorm(1000,4,1)
	Var5<-rnorm(1000,4,1)
	Var6<-rnorm(1000,4,1)
	Var7<-rnorm(1000,4,1)
	Var8<-rnorm(1000,4,1)
	Var9<-rnorm(1000,4,1)
	Var10<-rnorm(1000,4,1)
	Var11<-rnorm(1000,4,1)
	Var12<-rnorm(1000,4,1)
	Var13<-rnorm(1000,4,1)
	Var14<-rnorm(1000,4,1)
	Var15<-rnorm(1000,4,1)

	p.vals.too.small<-rep(0,15)
	DATA<-data.frame(y,Var1,Var2,Var3,Var4,Var5,Var6,Var7,Var8,Var9,Var10,Var11,Var12,Var13,Var14,Var15)
	for (j in (1:15))
	{
		if (summary(lm(y~DATA[,j+1],data=DATA))$coefficients[2,4] < 0.05) {p.vals.too.small[j]<-1}
	}
	P.VALS[i]<-sum(p.vals.too.small)
}
sum(P.VALS)/100

###Is this number around 0.05?  The answer is probably no!  That means that we will probably find a significant relationship when there isn't one
###if we do enough tests.  
###The idea is that if you repeat and experiment enough times, you will get a significant relationship by chance.


###To try and fix this problem, we can adjust the p-values.  They are just adjusted to be slightly bigger so that
###it becomes harder to find a significant result.


y<-rnorm(1000,5,1)
Var1<-rnorm(1000,4,1)
Var2<-rnorm(1000,4,1)
Var3<-rnorm(1000,4,1)
Var4<-rnorm(1000,4,1)
Var5<-rnorm(1000,4,1)
Var6<-rnorm(1000,4,1)
Var7<-rnorm(1000,4,1)
Var8<-rnorm(1000,4,1)
Var9<-rnorm(1000,4,1)
Var10<-rnorm(1000,4,1)
Var11<-rnorm(1000,4,1)
Var12<-rnorm(1000,4,1)
Var13<-rnorm(1000,4,1)
Var14<-rnorm(1000,4,1)
Var15<-rnorm(1000,4,1)

DATA<-data.frame(y,Var1,Var2,Var3,Var4,Var5,Var6,Var7,Var8,Var9,Var10,Var11,Var12,Var13,Var14,Var15)

p.vals<-rep(0,15)
for (j in (1:15))
{
	p.vals[j]<-summary(lm(y~DATA[,j+1],data=DATA))$coefficients[2,4]
}

p.vals
p.adjust(p.vals, method="hommel")

###Compare the values of p.vals and p.adjust(p.vals, method="hommel").  You should find the latter set to all
###be larger than the first set.

###Does this solve the problem?

P.VALS<-rep(0,100)

for (i in (1:100))
{
	y<-rnorm(1000,5,1)
	Var1<-rnorm(1000,4,1)
	Var2<-rnorm(1000,4,1)
	Var3<-rnorm(1000,4,1)
	Var4<-rnorm(1000,4,1)
	Var5<-rnorm(1000,4,1)
	Var6<-rnorm(1000,4,1)
	Var7<-rnorm(1000,4,1)
	Var8<-rnorm(1000,4,1)
	Var9<-rnorm(1000,4,1)
	Var10<-rnorm(1000,4,1)
	Var11<-rnorm(1000,4,1)
	Var12<-rnorm(1000,4,1)
	Var13<-rnorm(1000,4,1)
	Var14<-rnorm(1000,4,1)
	Var15<-rnorm(1000,4,1)

	p.vals<-rep(0,15)
	DATA<-data.frame(y,Var1,Var2,Var3,Var4,Var5,Var6,Var7,Var8,Var9,Var10,Var11,Var12,Var13,Var14,Var15)
	for (j in (1:15))
	{
		p.vals[j]<-summary(lm(y~DATA[,j+1],data=DATA))$coefficients[2,4]
	}
	p.vals1<-p.adjust(p.vals, method="hommel")
	P.VALS[i]<-sum(p.vals1<0.05)
}
sum(P.VALS)/100

###Hopefully this number is around 0.05 now.

###Look up 
?p.adjust




